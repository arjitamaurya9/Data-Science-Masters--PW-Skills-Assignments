{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68dfd498",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb2af2",
   "metadata": {},
   "source": [
    "Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. There are many different ways to perform web scraping to obtain data from websites. These include using online services, particular API’s or even creating your code for web scraping from scratch.\n",
    "1. Lead Generation for Marketing - \n",
    "A web scraping software can be used to generate leads for marketing. Email and Phone lists for cold outreach can be built by scraping the data from relevant websites. For example, business contact details like phone number and email address can be scraped from yellow pages websites or from Google Maps business listings.\n",
    "2. E-Commerce - \n",
    "Web Scraping can be used to periodically extract data of products from various e-commerce websites like Amazon, eBay, Google Shopping etc. Product details like price, description, images, reviews, rating etc. can be easily extracted using a web scraping software.\n",
    "3. Data Analysis - \n",
    "You might want to collect and analyze data related to a specific category from multiple websites. The category might be real estate, automobiles, electronic gadgets, industrial equipment, business contacts, marketing etc. The different websites which belong to the specific category displays information in different formats. Even with a single website you may not be able to see all the data at once. The data may be spanned across multiple pages (like google search results, known as pagination or paginated lists) under various sections. Using a Web Scraper you can extract data from multiple websites to a single spreadsheet (or database) so that it becomes easy for you to analyze (or even visualize) the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b2ab1",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca2d703",
   "metadata": {},
   "source": [
    "The most common techniques used for Web Scraping are\n",
    "\n",
    "**Human Copy-and-Paste** \n",
    "\n",
    "Manually copying and pasting data from a web page into a text file or spreadsheet is the most basic form of web scraping. Even the best web-scraping technology cannot always replace a human’s manual examination and copy-and-paste, and this may be the only viable option when the websites for scraping explicitly prohibit machine automation.\n",
    "\n",
    "**Text Pattern Matching** \n",
    "\n",
    "The UNIX grep command or regular expression-matching facilities of programming languages can be used to extract information from web pages in a simple yet powerful way (for instance Perl or Python).\n",
    "\n",
    "**HTTP Programming** \n",
    "\n",
    "Static and dynamic web pages can be retrieved by using socket programming to send HTTP requests to a remote web server.\n",
    "\n",
    "**HTML Parsing** \n",
    "\n",
    "Many websites contain large collections of pages that are dynamically generated from an underlying structured source, such as a database. A common script or template is typically used to encode data from the same category into similar pages. A wrapper is a program in data mining that detects such templates in a specific information source, extracts its content, and converts it to a relational form.\n",
    "Wrapper generation algorithms assume that the input pages of a wrapper induction system follow a common template and can be identified using a URL common scheme. [2] Furthermore, semi-structured data query languages such as XQuery and HTQL can be used to parse HTML pages as well as retrieve and transform page content.\n",
    "\n",
    "**DOM Parsing** \n",
    "\n",
    "More information: Object Model for Documents, Programs can retrieve dynamic content generated by client-side scripts by embedding a full-fledged web browser, such as Internet Explorer or the Mozilla browser control. These browser controls also parse web pages into a DOM tree, which programs can use to retrieve portions of the pages. The resulting DOM tree can be parsed using languages such as Xpath.\n",
    "\n",
    "**Vertical Aggregation**\n",
    "\n",
    "Several companies have created vertically specific harvesting platforms. These platforms generate and monitor a plethora of “bots” for specific verticals with no “man in the loop” (direct human involvement) and no work related to a specific target site. The preparation entails creating a knowledge base for the entire vertical, after which the platform will create the bots automatically.\n",
    "The robustness of the platform is measured by the quality of the information it retrieves (typically the number of fields) and its scalability (how quickly it can scale up to hundreds or thousands of sites). This scalability is primarily used to target the Long Tail of sites that common aggregators find too difficult or time-consuming to harvest content from.\n",
    "\n",
    "**Semantic Annotation Recognizing** \n",
    "\n",
    "The scraped pages may include metadata, semantic markups, and annotations that can be used to locate specific data snippets. This technique can be viewed as a subset of DOM parsing if the annotations are embedded in the pages, as Microformat does. In another case, the annotations are stored and managed separately from the web pages, so scrapers can retrieve data schema and instructions from this layer before scraping the pages.\n",
    "\n",
    "**Computer Vision Web-Page Analysis**\n",
    "\n",
    "There are efforts using machine learning and computer vision to identify and extract information from web pages by visually interpreting pages as a human would.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d135e1",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a5151",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). \n",
    "It creates a parse tree for parsed pages that can be used to extract data from HTML,which is useful for web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec1a0c1",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985ef2c",
   "metadata": {},
   "source": [
    "Flask is a web framework that is commonly used to build web applications and APIs in Python. While it is not required for web scraping, it can be useful in certain scenarios.\n",
    "In the context of a web scraping project, Flask is used to create a simple web interface that allows users to input URLs or search queries and view the scraped data in a user-friendly format. For example, you could create a Flask app that takes a search query for a product on Flipkart, scrapes the results page, and displays the relevant information (such as product names, prices, and ratings) in a table on the app's homepage.\n",
    "Using Flask in this way can make it easier to share the results of your web scraping project with others, as they can access the scraped data through a web interface rather than needing to run the scraping code themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0446ae3d",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eac525",
   "metadata": {},
   "source": [
    "**What is the use of CodePipeline in AWS?**\n",
    "AWS CodePipeline is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. You can quickly model and configure the different stages of a software release process. CodePipeline automates the steps required to release your software changes continuously.\n",
    "\n",
    "**Why AWS Elastic Beanstalk?**\n",
    "Elastic Beanstalk is a service for deploying and scaling web applications and services. Upload your code and Elastic Beanstalk automatically handles the deployment—from capacity provisioning, load balancing, and auto scaling to application health monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f17ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
